================================================================================
데이터 증강 프롬프트 가이드
================================================================================
프로젝트: Farm-Flow IoT 공격 분류 LLM 학습
날짜: 2025-12-15
목적: GPT-4o-mini를 활용한 3가지 데이터 증강 전략 상세 문서화

================================================================================
1. 원본 GPT 생성 프롬프트 (create_gpt_training_data.py)
================================================================================

[목적]
- 347,685개 원본 샘플에 대해 상세한 reasoning 생성
- 29개 feature 값을 기반으로 800-1000 단어 분석 제공
- 클래스당 균등 샘플링 (각 57,611개 또는 2,019개)

[System Prompt]
---------------------------------------------------------------------------------
You are an expert cybersecurity analyst. You MUST reference specific numerical 
values from the provided data in your analysis. Be precise and technical.
---------------------------------------------------------------------------------

[User Prompt Template]
---------------------------------------------------------------------------------
You are an expert cybersecurity analyst analyzing IoT network traffic.

**Given Traffic Sample - Class: {class_name}**

**All Network Features (normalized Z-score values):**
{feature_text}
  - missed_bytes: 0.123456
  - orig_pkts: 1.234567
  - orig_ip_bytes: 2.345678
  ... (총 29개 feature)

**CRITICAL INSTRUCTIONS:**
1. YOU MUST reference SPECIFIC FEATURE VALUES from the data above in your explanation
2. Explain WHY each specific value indicates "{class_name}" classification
3. Compare these values to expected ranges for this attack type
4. Be highly technical and precise - use the ACTUAL numbers shown above
5. YOUR RESPONSE MUST BE 800-1000 WORDS to provide comprehensive analysis
6. COMPLETE ALL SENTENCES - do not end abruptly or leave sentences unfinished

**Your Task:**
Provide a detailed technical analysis (800-1000 words) explaining why THIS 
SPECIFIC traffic sample is classified as "{class_name}". 

**Requirements:**
- Start each point by stating the EXACT feature name and its EXACT value shown above
- Explain what that specific value means for this classification (2-3 sentences per feature)
- Reference 8-12 key features with their actual values
- Compare values to normal traffic and other attack types
- Explain the attack mechanism or normal behavior pattern in detail
- Conclude with a summary of the overall attack signature

**EXACT FORMAT YOU MUST FOLLOW:**

- **feature_name = exact_value**: [Technical explanation 2-3 sentences describing 
  what this value means, why it indicates {class_name}, and how it compares to 
  other traffic types.]

- **another_feature = exact_value**: [Detailed analysis 2-3 sentences explaining 
  the significance of this specific value in the context of {class_name} 
  classification.]

[Continue for 8-12 features]

- **Overall Pattern**: [Final paragraph 3-4 sentences summarizing how all these 
  features together create the distinctive signature of {class_name} attack/traffic.]

**COMPLETE EXAMPLE (FOLLOW THIS STRUCTURE):**

- **resp_pkts = 2.145678**: This high positive value indicates strong bidirectional 
  communication with significantly elevated response packet count. In a TCP_Flood 
  attack, this occurs because the target server attempts to respond to each malicious 
  SYN packet with SYN-ACK packets, creating an abnormal response pattern. Normal 
  traffic typically shows resp_pkts values between -0.5 and 0.5, making this value 
  highly indicative of flooding behavior.

- **fwd_header_size_min = -1.234567**: This negative value below -1.0 is uniquely 
  characteristic of Port_Scanning attacks where minimal packet headers are used. 
  The attacker sends streamlined packets to probe multiple ports rapidly, resulting 
  in unusually small header sizes. This value is rarely seen in legitimate traffic 
  or other attack types, serving as a strong discriminator for Port_Scanning 
  classification.

- **Overall Pattern**: The combination of elevated response packets, abnormal header 
  sizes, and characteristic timing patterns creates a distinctive signature for 
  {class_name}. These features collectively indicate malicious intent rather than 
  normal network behavior, with statistical values far outside typical operational ranges.

REMEMBER: Write 800-1000 words, reference 8-12 features, complete all sentences, 
and follow the exact format above.
---------------------------------------------------------------------------------

[API 설정]
- Model: gpt-4o-mini
- Temperature: 0.7
- Max tokens: 1024
- Top_p: 0.95

[처리 방식]
- 클래스별 순차 처리: TCP_Flood → Port_Scanning → ICMP_Flood → Arp_Spoofing 
  → MQTT_Flood → HTTP_Flood → Normal
- 각 클래스에서 랜덤 샘플링
- JSONL 형식 저장 (즉시 학습 가능)

[결과]
- 총 347,685개 샘플 생성
- 소요 시간: ~6.5시간 (40 workers 병렬)
- 비용: $150
- 문제점: 85.31% 중복으로 인한 다양성 부족
- Training Loss: 0.3440 (WORST)


================================================================================
2. Paraphrasing 증강 (paraphrase_duplicates.py)
================================================================================

[목적]
- 85.31% 중복된 원본 reasoning의 표현만 다양화
- Feature 값은 그대로, reasoning 문장만 paraphrasing
- 내용/의미는 동일하게 유지하면서 표현 변경

[전략]
1. Reasoning 기반 중복 탐지 (MD5 해시)
2. 각 중복 그룹에서 첫 번째는 원본 유지
3. 나머지는 GPT로 paraphrasing

[System Prompt]
---------------------------------------------------------------------------------
You are an expert technical writer. Paraphrase the given text while keeping 
the exact same meaning, technical details, content, and conclusion. Only change 
the wording and sentence structure for variety. Keep the length approximately 
the same as the original.
---------------------------------------------------------------------------------

[User Prompt Template]
---------------------------------------------------------------------------------
Paraphrase this IoT security analysis. 

IMPORTANT: 
- Keep all technical details, numbers, content, and the classification EXACTLY the same
- Only vary the expression
- Keep approximately {original_length} characters

Original analysis:
{original_reasoning}

Paraphrased version (keep same length ~{original_length} chars, keep 
CLASSIFICATION line exactly as is):
---------------------------------------------------------------------------------

[API 설정]
- Model: gpt-4o-mini
- Temperature: 0.8 (높은 다양성 확보)
- Max tokens: 2048

[처리 방식]
- 40 Workers 병렬 처리
- 100개마다 자동 저장 (train_data_paraphrased.jsonl)
- 최대 5회 재시도 (API 오류 대응)

[중복 탐지 로직]
```python
# CLASSIFICATION 제외하고 reasoning만 해싱
reasoning_only = re.sub(r'CLASSIFICATION:.*', '', assistant_content, 
                        flags=re.DOTALL).strip()
reasoning_hash = hashlib.md5(reasoning_only.encode()).hexdigest()
```

[결과]
- 완성도: 80.0% (222,623 / 278,148)
- 10K Loss: 0.1003 ⭐ BEST PERFORMANCE
- 30K Loss: 0.1069
- 비용: ~$200
- 개선: Original 대비 3.4배 성능 향상


================================================================================
3. Feature Variation 증강 (paraphrase_with_feature_variation.py)
================================================================================

[목적]
- Feature 값 자체를 변형 (±1-5% 노이즈)
- 변형된 feature에 맞게 reasoning 재생성
- 더 근본적인 데이터 다양성 확보

[전략]
1. Feature 기반 중복 탐지 (User content 해시)
2. 각 중복 그룹에서 첫 번째는 원본 유지
3. 나머지는 Feature 변형 + Reasoning 재생성

[Feature 변형 로직]
---------------------------------------------------------------------------------
def add_noise_to_features(feature_text):
    """
    Feature 값에 ±1-5% 노이즈 추가
    - Z-score 정규화 값 (-3 ~ 3 범위)에 대해 작은 변동
    """
    for line in feature_text.split('\n'):
        if ':' in line:
            feature_name, original_value = line.split(':')
            original_value = float(original_value.strip())
            
            # ±5% 범위 랜덤 노이즈
            noise_factor = random.uniform(0.95, 1.05)
            new_value = original_value * noise_factor
            
            modified_line = f"{feature_name}: {new_value:.3f}"
---------------------------------------------------------------------------------

[System Prompt]
---------------------------------------------------------------------------------
You are an IoT network security expert. Analyze the given network traffic 
features and provide detailed reasoning for classification.
---------------------------------------------------------------------------------

[User Prompt Template]
---------------------------------------------------------------------------------
Analyze these IoT network traffic features and provide reasoning similar to 
this example, but adjusted for the new feature values:

Example reasoning format:
{original_reasoning}

New features to analyze:
{modified_features}

Provide detailed analysis with ANALYSIS, KEY_INDICATORS, and CLASSIFICATION sections:
---------------------------------------------------------------------------------

[API 설정]
- Model: gpt-4o-mini
- Temperature: 0.7
- Max tokens: 2048

[처리 방식]
- 40 Workers 병렬 처리
- 100개마다 자동 저장 (train_data_feature_varied.jsonl)
- Feature noise: random.uniform(0.95, 1.05) → ±5%

[중복 탐지 로직]
```python
# User content (feature 값) 기반 해싱
user_content = sample['messages'][0]['content']
feature_hash = hashlib.md5(user_content.encode()).hexdigest()
```

[결과]
- 완성도: 100%+ (278,148+)
- 10K Loss: 0.1139 (Paraphrasing보다 13.6% 높음)
- 30K Loss: 0.1569
- 비용: ~$211
- 특징: Feature도 변형되지만 성능은 Paraphrasing보다 낮음


================================================================================
4. 증강 전략 비교
================================================================================

┌──────────────────┬─────────────┬──────────┬─────────────────────────────┐
│ 방법             │ 완성도      │ Loss     │ 특징                        │
├──────────────────┼─────────────┼──────────┼─────────────────────────────┤
│ Original GPT     │ 100%        │ 0.3440   │ 85% 중복, 다양성 부족       │
├──────────────────┼─────────────┼──────────┼─────────────────────────────┤
│ Paraphrasing ⭐  │ 80%         │ 0.1003   │ 표현만 변경, 최고 성능      │
├──────────────────┼─────────────┼──────────┼─────────────────────────────┤
│ Feature Varied   │ 100%+       │ 0.1139   │ Feature+Reasoning 변형      │
└──────────────────┴─────────────┴──────────┴─────────────────────────────┘

[핵심 인사이트]
1. 원본 데이터 85.31% 중복 → 증강 필요성 입증
2. Paraphrasing이 Feature Variation보다 효과적
   - 이유: Feature 값의 통계적 패턴 보존이 중요
3. 80% 완성도에서 최고 성능 달성
   - 과도한 증강보다 품질 높은 일부 증강이 효과적


================================================================================
5. Progressive Training Strategy
================================================================================

[단계별 학습 전략]
- 10K:  초기 개념 학습, 빠른 실험
- 30K:  패턴 강화, 중간 평가
- 70K:  일반화 능력 향상
- 150K: 대규모 학습
- ALL:  전체 데이터 (278,148개)

[각 단계별 증강 현황]

Paraphrasing Track:
┌─────────┬────────────┬──────────┬──────────┐
│ Stage   │ Target     │ Complete │ Percent  │
├─────────┼────────────┼──────────┼──────────┤
│ 10K     │ 10,000     │ 10,000   │ 100.0%   │
│ 30K     │ 30,000     │ 27,233   │  90.8%   │
│ 70K     │ 70,000     │ 59,779   │  85.4%   │
│ 150K    │ 150,000    │ 123,367  │  82.2%   │
│ ALL     │ 278,148    │ 222,623  │  80.0%   │
└─────────┴────────────┴──────────┴──────────┘

Feature Variation Track:
┌─────────┬────────────┬──────────┬──────────┐
│ Stage   │ Target     │ Complete │ Percent  │
├─────────┼────────────┼──────────┼──────────┤
│ 10K     │ 10,000     │ 10,000   │ 100.0%   │
│ 30K     │ 30,000     │ 30,000   │ 100.0%   │
│ 70K     │ 70,000     │ 70,000   │ 100.0%   │
│ 150K    │ 150,000    │ 120,594  │  80.4%   │
│ ALL     │ 278,148    │ 278,148+ │ 100%+    │
└─────────┴────────────┴──────────┴──────────┘


================================================================================
6. 비용 및 시간 분석
================================================================================

[GPT API 비용]
- Original Generation:     $150 (347,685 samples, 40 workers, 6.5시간)
- Paraphrasing:            $200 (222,623 samples)
- Feature Variation:       $211 (278,148+ samples)
- Total:                   $561

[GPU 학습 비용 (H100 80GB)]
- 시간당 비용: $2-3/hour
- 예상 총 학습 시간: 720-2,200시간 (전체 모델)
- 예상 GPU 비용: $1,440-$4,400

[총 투자]
- 데이터 생성: $561
- GPU 학습 (예상): $1,440-$4,400
- 합계: ~$2,000-$5,000


================================================================================
7. 프롬프트 설계 원칙
================================================================================

[1] 구체성 (Specificity)
- ❌ "Analyze the traffic"
- ✅ "Reference EXACT feature values: resp_pkts = 2.145678"

[2] 길이 제약 (Length Control)
- ❌ "Provide analysis"
- ✅ "Write 800-1000 words, approximately {original_length} characters"

[3] 형식 강제 (Format Enforcement)
- ❌ "Explain the features"
- ✅ "EXACT FORMAT: - **feature = value**: [explanation]"

[4] 예시 제공 (Example-driven)
- ❌ "Write technical analysis"
- ✅ "COMPLETE EXAMPLE: - **resp_pkts = 2.145678**: This high positive value..."

[5] 반복 강조 (Repetition)
- "YOU MUST reference SPECIFIC FEATURE VALUES"
- "Keep the length approximately the same"
- "REMEMBER: Write 800-1000 words"

[6] 온도 조절 (Temperature Tuning)
- Original Generation: 0.7 (balanced creativity)
- Paraphrasing: 0.8 (high diversity)
- Feature Variation: 0.7 (controlled generation)


================================================================================
8. 실험 결과 요약
================================================================================

[성능 비교 - Training Loss (10K 기준)]
┌──────────────────────┬───────────┬─────────────┐
│ Method               │ Loss      │ Improvement │
├──────────────────────┼───────────┼─────────────┤
│ Original GPT         │ 0.3440    │ Baseline    │
│ Feature Variation    │ 0.1139    │ 3.0x better │
│ Paraphrasing ⭐      │ 0.1003    │ 3.4x better │
└──────────────────────┴───────────┴─────────────┘

[핵심 발견]
1. **원본 데이터 품질 문제 확인**
   - 85.31% 중복 (296,614 / 347,685)
   - HTTP_Flood: 97.23% 중복 (최악)
   - TCP_Flood: 46.19% 중복 (최선)

2. **Paraphrasing이 최적**
   - Feature 통계 보존이 중요
   - 표현 다양성만으로도 충분한 개선

3. **80% 증강으로 최고 성능**
   - 과도한 증강 불필요
   - 품질 > 양

4. **Progressive 전략 유효**
   - 10K → 30K 단계적 학습
   - 빠른 실험과 평가 가능


================================================================================
9. 재사용 가이드
================================================================================

[이 프롬프트를 다른 프로젝트에 적용하려면]

Step 1: Domain-specific 용어 변경
- "IoT network traffic" → 자신의 도메인
- "cybersecurity analyst" → 적절한 전문가 역할
- Feature 이름들을 자신의 데이터 컬럼명으로 변경

Step 2: 길이 조정
- 800-1000 words → 데이터셋 복잡도에 맞게 조정
- max_tokens: 1024-2048 범위에서 실험

Step 3: Temperature 실험
- 창의적 설명: 0.7-0.9
- 정확한 분류: 0.3-0.5
- Paraphrasing: 0.8-1.0

Step 4: 병렬 처리 규모
- API rate limit 확인
- 40 workers → 비용과 속도 균형

Step 5: 증강 비율 결정
- 중복 분석 먼저 수행
- 80-100% 범위에서 실험
- Loss curve 모니터링


================================================================================
10. 참고 파일
================================================================================

코드 파일:
- create_gpt_training_data.py           : 원본 GPT 생성
- paraphrase_duplicates.py              : Paraphrasing 증강
- paraphrase_with_feature_variation.py  : Feature Variation 증강

데이터 파일:
- train_data_original_size.jsonl        : 원본 GPT 생성 결과
- train_data_paraphrased.jsonl          : Paraphrasing 결과
- train_data_feature_varied.jsonl       : Feature Variation 결과

분석 스크립트:
- check_data_leakage.py                 : 중복 분석
- compare_raw_samples.py                : 샘플 비교
- analyze_failed_samples.py             : 실패 샘플 분석

학습 스크립트:
- train_paraphrased_10K.py
- train_paraphrased_30K.py
- train_feature_varied_10K.py
- train_feature_varied_30K.py

================================================================================
문서 끝
================================================================================
