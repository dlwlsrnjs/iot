================================================================================
Data Augmentation Prompt Guide
================================================================================
Project: Farm-Flow IoT Attack Classification LLM Training
Date: 2025-12-15
Purpose: Detailed documentation of 3 data augmentation strategies using GPT-4o-mini

================================================================================
1. Original GPT Generation Prompt (create_gpt_training_data.py)
================================================================================

[Purpose]
- Generate detailed reasoning for 347,685 original samples
- Provide 800-1000 word analysis based on 29 feature values
- Balanced sampling per class (57,611 or 2,019 each)

[System Prompt]
---------------------------------------------------------------------------------
You are an expert cybersecurity analyst. You MUST reference specific numerical 
values from the provided data in your analysis. Be precise and technical.
---------------------------------------------------------------------------------

[User Prompt Template]
---------------------------------------------------------------------------------
You are an expert cybersecurity analyst analyzing IoT network traffic.

**Given Traffic Sample - Class: {class_name}**

**All Network Features (normalized Z-score values):**
{feature_text}
  - missed_bytes: 0.123456
  - orig_pkts: 1.234567
  - orig_ip_bytes: 2.345678
  ... (총 29개 feature)

**CRITICAL INSTRUCTIONS:**
1. YOU MUST reference SPECIFIC FEATURE VALUES from the data above in your explanation
2. Explain WHY each specific value indicates "{class_name}" classification
3. Compare these values to expected ranges for this attack type
4. Be highly technical and precise - use the ACTUAL numbers shown above
5. YOUR RESPONSE MUST BE 800-1000 WORDS to provide comprehensive analysis
6. COMPLETE ALL SENTENCES - do not end abruptly or leave sentences unfinished

**Your Task:**
Provide a detailed technical analysis (800-1000 words) explaining why THIS 
SPECIFIC traffic sample is classified as "{class_name}". 

**Requirements:**
- Start each point by stating the EXACT feature name and its EXACT value shown above
- Explain what that specific value means for this classification (2-3 sentences per feature)
- Reference 8-12 key features with their actual values
- Compare values to normal traffic and other attack types
- Explain the attack mechanism or normal behavior pattern in detail
- Conclude with a summary of the overall attack signature

**EXACT FORMAT YOU MUST FOLLOW:**

- **feature_name = exact_value**: [Technical explanation 2-3 sentences describing 
  what this value means, why it indicates {class_name}, and how it compares to 
  other traffic types.]

- **another_feature = exact_value**: [Detailed analysis 2-3 sentences explaining 
  the significance of this specific value in the context of {class_name} 
  classification.]

[Continue for 8-12 features]

- **Overall Pattern**: [Final paragraph 3-4 sentences summarizing how all these 
  features together create the distinctive signature of {class_name} attack/traffic.]

**COMPLETE EXAMPLE (FOLLOW THIS STRUCTURE):**

- **resp_pkts = 2.145678**: This high positive value indicates strong bidirectional 
  communication with significantly elevated response packet count. In a TCP_Flood 
  attack, this occurs because the target server attempts to respond to each malicious 
  SYN packet with SYN-ACK packets, creating an abnormal response pattern. Normal 
  traffic typically shows resp_pkts values between -0.5 and 0.5, making this value 
  highly indicative of flooding behavior.

- **fwd_header_size_min = -1.234567**: This negative value below -1.0 is uniquely 
  characteristic of Port_Scanning attacks where minimal packet headers are used. 
  The attacker sends streamlined packets to probe multiple ports rapidly, resulting 
  in unusually small header sizes. This value is rarely seen in legitimate traffic 
  or other attack types, serving as a strong discriminator for Port_Scanning 
  classification.

- **Overall Pattern**: The combination of elevated response packets, abnormal header 
  sizes, and characteristic timing patterns creates a distinctive signature for 
  {class_name}. These features collectively indicate malicious intent rather than 
  normal network behavior, with statistical values far outside typical operational ranges.

REMEMBER: Write 800-1000 words, reference 8-12 features, complete all sentences, 
and follow the exact format above.
---------------------------------------------------------------------------------

[API 설정]
- Model: gpt-4o-mini
- Temperature: 0.7
- Max tokens: 1024
- Top_p: 0.95

[Processing Method]
- Sequential class processing: TCP_Flood → Port_Scanning → ICMP_Flood → Arp_Spoofing 
  → MQTT_Flood → HTTP_Flood → Normal
- Random sampling from each class
- Save in JSONL format (ready for training)

[Results]
- Total 347,685 samples generated
- Time: ~6.5 hours (40 workers parallel)
- Cost: $150
- Issue: 85.31% duplication causing lack of diversity
- Training Loss: 0.3440 (WORST)


================================================================================
2. Paraphrasing Augmentation (paraphrase_duplicates.py)
================================================================================

[Purpose]
- Diversify expressions of 85.31% duplicated original reasoning
- Keep feature values intact, only paraphrase reasoning sentences
- Maintain content/meaning while varying expression

[Strategy]
1. Detect duplicates based on reasoning (MD5 hash)
2. Keep first sample in each duplicate group as original
3. Paraphrase the rest using GPT

[System Prompt]
---------------------------------------------------------------------------------
You are an expert technical writer. Paraphrase the given text while keeping 
the exact same meaning, technical details, content, and conclusion. Only change 
the wording and sentence structure for variety. Keep the length approximately 
the same as the original.
---------------------------------------------------------------------------------

[User Prompt Template]
---------------------------------------------------------------------------------
Paraphrase this IoT security analysis. 

IMPORTANT: 
- Keep all technical details, numbers, content, and the classification EXACTLY the same
- Only vary the expression
- Keep approximately {original_length} characters

Original analysis:
{original_reasoning}

Paraphrased version (keep same length ~{original_length} chars, keep 
CLASSIFICATION line exactly as is):
---------------------------------------------------------------------------------

[API Settings]
- Model: gpt-4o-mini
- Temperature: 0.8 (high diversity)
- Max tokens: 2048

[Processing Method]
- 40 Workers parallel processing
- Auto-save every 100 samples (train_data_paraphrased.jsonl)
- Up to 5 retries (API error handling)

[Duplicate Detection Logic]
```python
# CLASSIFICATION 제외하고 reasoning만 해싱
reasoning_only = re.sub(r'CLASSIFICATION:.*', '', assistant_content, 
                        flags=re.DOTALL).strip()
reasoning_hash = hashlib.md5(reasoning_only.encode()).hexdigest()
```

[Results]
- Completion: 80.0% (222,623 / 278,148)
- 10K Loss: 0.1003 ⭐ BEST PERFORMANCE
- 30K Loss: 0.1069
- Cost: ~$200
- Improvement: 3.4x better than Original


================================================================================
3. Feature Variation Augmentation (paraphrase_with_feature_variation.py)
================================================================================

[Purpose]
- Modify feature values themselves (±1-5% noise)
- Regenerate reasoning based on modified features
- Achieve more fundamental data diversity

[Strategy]
1. Detect duplicates based on features (User content hash)
2. Keep first sample in each duplicate group as original
3. Feature modification + Reasoning regeneration for the rest

[Feature 변형 로직]
---------------------------------------------------------------------------------
def add_noise_to_features(feature_text):
    """
    Feature 값에 ±1-5% 노이즈 추가
    - Z-score 정규화 값 (-3 ~ 3 범위)에 대해 작은 변동
    """
    for line in feature_text.split('\n'):
        if ':' in line:
            feature_name, original_value = line.split(':')
            original_value = float(original_value.strip())
            
            # ±5% 범위 랜덤 노이즈
            noise_factor = random.uniform(0.95, 1.05)
            new_value = original_value * noise_factor
            
            modified_line = f"{feature_name}: {new_value:.3f}"
---------------------------------------------------------------------------------

[System Prompt]
---------------------------------------------------------------------------------
You are an IoT network security expert. Analyze the given network traffic 
features and provide detailed reasoning for classification.
---------------------------------------------------------------------------------

[User Prompt Template]
---------------------------------------------------------------------------------
Analyze these IoT network traffic features and provide reasoning similar to 
this example, but adjusted for the new feature values:

Example reasoning format:
{original_reasoning}

New features to analyze:
{modified_features}

Provide detailed analysis with ANALYSIS, KEY_INDICATORS, and CLASSIFICATION sections:
---------------------------------------------------------------------------------

[API 설정]
- Model: gpt-4o-mini
- Temperature: 0.7
- Max tokens: 2048

[처리 방식]
- 40 Workers 병렬 처리
- 100개마다 자동 저장 (train_data_feature_varied.jsonl)
- Feature noise: random.uniform(0.95, 1.05) → ±5%

[중복 탐지 로직]
```python
# User content (feature 값) 기반 해싱
user_content = sample['messages'][0]['content']
feature_hash = hashlib.md5(user_content.encode()).hexdigest()
```

[Results]
- Completion: 100%+ (278,148+)
- 10K Loss: 0.1139 (13.6% higher than Paraphrasing)
- 30K Loss: 0.1569
- Cost: ~$211
- Note: Features modified but performance lower than Paraphrasing


================================================================================
4. Augmentation Strategy Comparison
================================================================================

┌──────────────────┬─────────────┬──────────┬─────────────────────────────┐
│ Method           │ Completion  │ Loss     │ Features                    │
├──────────────────┼─────────────┼──────────┼─────────────────────────────┤
│ Original GPT     │ 100%        │ 0.3440   │ 85% dup, low diversity      │
├──────────────────┼─────────────┼──────────┼─────────────────────────────┤
│ Paraphrasing ⭐  │ 80%         │ 0.1003   │ Expression only, best       │
├──────────────────┼─────────────┼──────────┼─────────────────────────────┤
│ Feature Varied   │ 100%+       │ 0.1139   │ Feature+Reasoning modified  │
└──────────────────┴─────────────┴──────────┴─────────────────────────────┘

[Key Insights]
1. 85.31% duplication in original data → proves need for augmentation
2. Paraphrasing more effective than Feature Variation
   - Reason: Preserving statistical patterns in features is important
3. Best performance at 80% completion
   - Quality over quantity: focused augmentation beats excessive augmentation


================================================================================
5. Progressive Training Strategy
================================================================================

[Stage-wise Learning Strategy]
- 10K:  Initial concept learning, rapid experiments
- 30K:  Pattern reinforcement, mid-stage evaluation
- 70K:  Generalization capability improvement
- 150K: Large-scale learning
- ALL:  Full dataset (278,148 samples)

[Augmentation Status by Stage]

Paraphrasing Track:
┌─────────┬────────────┬──────────┬──────────┐
│ Stage   │ Target     │ Complete │ Percent  │
├─────────┼────────────┼──────────┼──────────┤
│ 10K     │ 10,000     │ 10,000   │ 100.0%   │
│ 30K     │ 30,000     │ 27,233   │  90.8%   │
│ 70K     │ 70,000     │ 59,779   │  85.4%   │
│ 150K    │ 150,000    │ 123,367  │  82.2%   │
│ ALL     │ 278,148    │ 222,623  │  80.0%   │
└─────────┴────────────┴──────────┴──────────┘

Feature Variation Track:
┌─────────┬────────────┬──────────┬──────────┐
│ Stage   │ Target     │ Complete │ Percent  │
├─────────┼────────────┼──────────┼──────────┤
│ 10K     │ 10,000     │ 10,000   │ 100.0%   │
│ 30K     │ 30,000     │ 30,000   │ 100.0%   │
│ 70K     │ 70,000     │ 70,000   │ 100.0%   │
│ 150K    │ 150,000    │ 120,594  │  80.4%   │
│ ALL     │ 278,148    │ 278,148+ │ 100%+    │
└─────────┴────────────┴──────────┴──────────┘


================================================================================
6. Cost and Time Analysis
================================================================================

[GPT API Cost]
- Original Generation:     $150 (347,685 samples, 40 workers, 6.5 hours)
- Paraphrasing:            $200 (222,623 samples)
- Feature Variation:       $211 (278,148+ samples)
- Total:                   $561

[GPU Training Cost (H100 80GB)]
- Cost per hour: $2-3/hour
- Estimated total training time: 720-2,200 hours (all models)
- Estimated GPU cost: $1,440-$4,400

[Total Investment]
- Data generation: $561
- GPU training (estimated): $1,440-$4,400
- Total: ~$2,000-$5,000


================================================================================
7. Prompt Design Principles
================================================================================

[1] Specificity
- ❌ "Analyze the traffic"
- ✅ "Reference EXACT feature values: resp_pkts = 2.145678"

[2] Length Control
- ❌ "Provide analysis"
- ✅ "Write 800-1000 words, approximately {original_length} characters"

[3] Format Enforcement
- ❌ "Explain the features"
- ✅ "EXACT FORMAT: - **feature = value**: [explanation]"

[4] Example-driven
- ❌ "Write technical analysis"
- ✅ "COMPLETE EXAMPLE: - **resp_pkts = 2.145678**: This high positive value..."

[5] Repetition
- "YOU MUST reference SPECIFIC FEATURE VALUES"
- "Keep the length approximately the same"
- "REMEMBER: Write 800-1000 words"

[6] Temperature Tuning
- Original Generation: 0.7 (balanced creativity)
- Paraphrasing: 0.8 (high diversity)
- Feature Variation: 0.7 (controlled generation)


================================================================================
8. Experimental Results Summary
================================================================================

[Performance Comparison - Training Loss (10K baseline)]
┌──────────────────────┬───────────┬─────────────┐
│ Method               │ Loss      │ Improvement │
├──────────────────────┼───────────┼─────────────┤
│ Original GPT         │ 0.3440    │ Baseline    │
│ Feature Variation    │ 0.1139    │ 3.0x better │
│ Paraphrasing ⭐      │ 0.1003    │ 3.4x better │
└──────────────────────┴───────────┴─────────────┘

[Key Findings]
1. **Original Data Quality Issues Confirmed**
   - 85.31% duplication (296,614 / 347,685)
   - HTTP_Flood: 97.23% duplicate (worst)
   - TCP_Flood: 46.19% duplicate (best)

2. **Paraphrasing is Optimal**
   - Preserving feature statistics is important
   - Expression diversity alone provides sufficient improvement

3. **Best Performance at 80% Augmentation**
   - Excessive augmentation unnecessary
   - Quality > Quantity

4. **Progressive Strategy Validated**
   - 10K → 30K staged learning
   - Enables rapid experimentation and evaluation


================================================================================
9. 재사용 가이드
================================================================================

[이 프롬프트를 다른 프로젝트에 적용하려면]

Step 1: Domain-specific 용어 변경
- "IoT network traffic" → 자신의 도메인
- "cybersecurity analyst" → 적절한 전문가 역할
- Feature 이름들을 자신의 데이터 컬럼명으로 변경

Step 2: 길이 조정
- 800-1000 words → 데이터셋 복잡도에 맞게 조정
- max_tokens: 1024-2048 범위에서 실험

Step 3: Experiment with Temperature
- Creative explanation: 0.7-0.9
- Accurate classification: 0.3-0.5
- Paraphrasing: 0.8-1.0

Step 4: Parallel Processing Scale
- Check API rate limits
- 40 workers → balance cost and speed

Step 5: Determine Augmentation Ratio
- Perform duplication analysis first
- Experiment in 80-100% range
- Monitor loss curve


================================================================================
10. Reference Files
================================================================================

Code Files:
- create_gpt_training_data.py           : Original GPT generation
- paraphrase_duplicates.py              : Paraphrasing augmentation
- paraphrase_with_feature_variation.py  : Feature Variation augmentation

Data Files:
- train_data_original_size.jsonl        : Original GPT generation results
- train_data_paraphrased.jsonl          : Paraphrasing results
- train_data_feature_varied.jsonl       : Feature Variation results

Analysis Scripts:
- check_data_leakage.py                 : Duplication analysis
- compare_raw_samples.py                : Sample comparison
- analyze_failed_samples.py             : Failed sample analysis

Training Scripts:
- train_paraphrased_10K.py
- train_paraphrased_30K.py
- train_feature_varied_10K.py
- train_feature_varied_30K.py

================================================================================
End of Document
================================================================================
