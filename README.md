# IoT Network Attack Classification with Explainable LLM

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![HuggingFace](https://img.shields.io/badge/ü§ó-HuggingFace-yellow.svg)](https://huggingface.co/)

> **Explainable AI for IoT Network Security**

A research project implementing explainable AI for IoT network attack classification using the Farm-Flow dataset. This system provides human-interpretable reasoning for each classification through fine-tuned large language models.

## üéØ Project Overview

### Goals
- **Explainability**: Generate human-readable reasoning for each classification
- **Efficiency**: Optimize for real-world deployment on consumer GPUs
- **Scalability**: Progressive training strategy for large datasets
- **Data Quality**: Address duplication issues through systematic augmentation

---

## üìä Dataset Information

### Farm-Flow IoT Network Security Dataset

**Source**: Published IoT security research paper (2024)
- **Total Samples**: 347,685 (Paper reports 405,296 - see [Data Mismatch Analysis](DATA_MISMATCH_SUMMARY.md))
- **Classes**: 7 attack types (Paper: 8 classes, UDP_Flood missing in our version)
- **Features**: 29 normalized network traffic features (Z-score)
- **Split**: 278,148 train / 69,537 validation (80:20)

### Attack Classes

| Class | Samples | Duplication Rate | Description |
|-------|---------|-----------------|-------------|
| **HTTP_Flood** | 57,611 | 97.23% | HTTP GET/POST flooding attacks |
| **MQTT_Flood** | 57,611 | 97.16% | MQTT protocol flooding |
| **Port_Scanning** | 57,611 | 95.53% | Port reconnaissance attacks |
| **Arp_Spoofing** | 57,566 | 91.39% | ARP cache poisoning |
| **ICMP_Flood** | 57,605 | 85.91% | Ping flood attacks |
| **TCP_Flood** | 57,611 | 46.19% | TCP SYN flooding |
| **Normal** | 2,019 | 41.16% | Legitimate traffic |

**Critical Finding**: 85.31% overall duplication (296,614 / 347,685) necessitated data augmentation strategy.

---

## üèóÔ∏è Architecture

### Model

- **Base Model**: [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct) (Alibaba)
- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)
  - Rank (r): 16
  - Alpha: 32
  - Dropout: 0.05
  - **Trainable Parameters**: 0.28% (6.9M / 2.4B)

### Training Configuration

```python
{
  "model": "Qwen/Qwen2.5-1.5B-Instruct",
  "method": "LoRA",
  "batch_size": 2,
  "gradient_accumulation_steps": 8,
  "effective_batch_size": 16,
  "epochs": 5,
  "learning_rate": 2e-4,
  "lr_scheduler": "cosine",
  "precision": "bfloat16",
  "hardware": "2x H100 80GB HBM3",
  "eval_steps": 500
}
```

### Data Augmentation Pipeline

**Problem**: Original dataset had 85.31% duplication ‚Üí poor training diversity

**Solution**: Three augmentation strategies tested

#### 1. Original GPT Generation
```python
# Prompt: 800-1000 word detailed analysis
# Temperature: 0.7
# Result: 347,685 samples generated
```

#### 2. Paraphrasing
```python
# Strategy: Vary expression while keeping features identical
# Temperature: 0.8 (high diversity)
# Result: 222,623 samples (80% of target)
```

#### 3. Feature Variation
```python
# Strategy: Add ¬±5% noise to features + regenerate reasoning
# Temperature: 0.7
# Result: 278,148+ samples (100%+ of target)
```

**Key Insight**: Preserving feature statistics (Paraphrasing) provides better quality than modifying features (Feature Variation)

---

## üìÅ Repository Structure

```
iot/
‚îú‚îÄ‚îÄ üìÑ README.md                              # This file
‚îú‚îÄ‚îÄ üìÑ RESEARCH_METHODOLOGY.txt               # Comprehensive methodology (1,633 lines)
‚îú‚îÄ‚îÄ üìÑ DATA_AUGMENTATION_PROMPTS.txt          # Detailed prompt engineering guide
‚îú‚îÄ‚îÄ üìÑ requirements_rag.txt                   # Python dependencies
‚îÇ
‚îú‚îÄ‚îÄ üìä Documentation
‚îÇ   ‚îú‚îÄ‚îÄ CLASSIFICATION_GUIDE_REPORT.md        # Classification strategies
‚îÇ   ‚îú‚îÄ‚îÄ DATABASE_QUALITY_REPORT.md            # Data quality analysis
‚îÇ   ‚îú‚îÄ‚îÄ DATA_MISMATCH_SUMMARY.md              # Dataset version discrepancy
‚îÇ   ‚îú‚îÄ‚îÄ HUGGINGFACE_MODELS.md                 # Model information
‚îÇ   ‚îî‚îÄ‚îÄ *.md                                  # 30+ analysis reports
‚îÇ
‚îú‚îÄ‚îÄ üîß Training Scripts
‚îÇ   ‚îú‚îÄ‚îÄ finetune_qwen.py                      # Main LoRA fine-tuning script
‚îÇ   ‚îú‚îÄ‚îÄ train_paraphrased_10K.py              # Paraphrasing 10K training
‚îÇ   ‚îú‚îÄ‚îÄ train_paraphrased_30K.py              # Paraphrasing 30K training
‚îÇ   ‚îú‚îÄ‚îÄ train_feature_varied_10K.py           # Feature Variation 10K
‚îÇ   ‚îú‚îÄ‚îÄ train_feature_varied_30K.py           # Feature Variation 30K
‚îÇ   ‚îú‚îÄ‚îÄ train_*_150K.py                       # 150K stage scripts
‚îÇ   ‚îú‚îÄ‚îÄ auto_train_*.sh                       # Automated training shells
‚îÇ   ‚îî‚îÄ‚îÄ run_progressive_all.sh                # Full progressive pipeline
‚îÇ
‚îú‚îÄ‚îÄ üìä Analysis Tools
‚îÇ   ‚îú‚îÄ‚îÄ automl_correct.py                     # AutoML baseline (96.95%)
‚îÇ   ‚îú‚îÄ‚îÄ check_data_leakage.py                 # Duplication analysis
‚îÇ   ‚îú‚îÄ‚îÄ compare_raw_samples.py                # Sample comparison
‚îÇ   ‚îú‚îÄ‚îÄ analyze_attack_patterns.py            # Pattern analysis
‚îÇ   ‚îî‚îÄ‚îÄ split_progressive_datasets.py         # Progressive stage splitter
‚îÇ
‚îú‚îÄ‚îÄ ‚öôÔ∏è Configuration
‚îÇ   ‚îú‚îÄ‚îÄ .gitignore                            # Excludes large files
‚îÇ   ‚îú‚îÄ‚îÄ .env.example                          # API key template
‚îÇ   ‚îî‚îÄ‚îÄ requirements_rag.txt                  # Dependencies
‚îÇ
‚îî‚îÄ‚îÄ üîí Excluded (API keys removed for security)
    ‚îú‚îÄ‚îÄ create_gpt_training_data.py           # Original data generation
    ‚îú‚îÄ‚îÄ paraphrase_duplicates.py              # Paraphrasing script
    ‚îú‚îÄ‚îÄ paraphrase_with_feature_variation.py  # Feature variation script
    ‚îú‚îÄ‚îÄ progressive_paraphrasing.py           # Progressive paraphrasing
    ‚îî‚îÄ‚îÄ progressive_feature_variation.py      # Progressive feature variation
```

---

## üöÄ Quick Start

### 1. Environment Setup

```bash
# Clone repository
git clone https://github.com/[AUTHOR]/iot.git
cd iot

# Install dependencies
pip install -r requirements_rag.txt

# Configure API keys (if using data generation)
cp .env.example .env
# Edit .env with your OpenAI API key
```

### 2. Run AutoML Baseline

```bash
python automl_correct.py
```

This establishes a baseline using traditional ML methods (RandomForest/WeightedEnsemble)

### 3. Data Augmentation (Optional - requires OpenAI API)

```bash
# Note: Data generation scripts excluded from repo for security
# See DATA_AUGMENTATION_PROMPTS.txt for detailed prompt engineering guide

# If you have the scripts:
# 1. Original generation: python create_gpt_training_data.py
# 2. Progressive paraphrasing: python progressive_paraphrasing.py
# 3. Progressive feature variation: python progressive_feature_variation.py
# 4. Split datasets: python split_progressive_datasets.py
```

### 4. Fine-tuning

```bash
# Train with Paraphrasing data (best performance)
python train_paraphrased_10K.py

# Train with Feature Variation data
python train_feature_varied_10K.py

# Or run full progressive pipeline
./run_progressive_all.sh
```

### 5. Evaluation

```bash
# Inference example
python inference_example.py

# Expected inference time:
# - Current (training): 47 seconds/sample
# - Optimized (deployment): 0.7-1 seconds/sample
```

---

## üìà Progressive Training Strategy

Training proceeds in stages to enable rapid experimentation and resource-efficient learning:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Stage    ‚îÇ Samples   ‚îÇ Purpose                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 10K      ‚îÇ 10,000    ‚îÇ Initial concept learning, fast     ‚îÇ
‚îÇ 30K      ‚îÇ 30,000    ‚îÇ Pattern reinforcement, evaluation  ‚îÇ
‚îÇ 70K      ‚îÇ 70,000    ‚îÇ Generalization improvement         ‚îÇ
‚îÇ 150K     ‚îÇ 150,000   ‚îÇ Large-scale learning               ‚îÇ
‚îÇ ALL      ‚îÇ 278,148   ‚îÇ Full dataset training              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Augmentation Completion Status

**Paraphrasing Track:**
```
10K:  10,000 / 10,000   (100.0%) ‚úÖ
30K:  27,233 / 30,000   ( 90.8%)
70K:  59,779 / 70,000   ( 85.4%)
150K: 123,367 / 150,000 ( 82.2%)
ALL:  222,623 / 278,148 ( 80.0%)
```

**Feature Variation Track:**
```
10K:  10,000 / 10,000   (100.0%) ‚úÖ
30K:  30,000 / 30,000   (100.0%) ‚úÖ
70K:  70,000 / 70,000   (100.0%) ‚úÖ
150K: 120,594 / 150,000 ( 80.4%)
ALL:  278,148+ / 278,148 (100%+) ‚úÖ
```

---

## ‚ö° Inference Optimization

### Current Performance

| Stage | Time/Sample | Notes |
|-------|------------|-------|
| **During Training** | 47 seconds | 5 models competing for GPU |
| **Dedicated GPU** | 6.7-9.4 seconds | Single model, no competition |
| **Target (Optimized)** | 0.7-1 seconds | With full optimization stack |

### Optimization Stack

```python
# Stage 1: GPU Dedication (5-7x speedup)
# Stage 2: Batch Processing (2-4x speedup)
batch_size = 32  # for H100, 16 for RTX 3060

# Stage 3: Token Reduction (2-3x speedup)
max_new_tokens = 256  # down from 512

# Stage 4: Quantization (1.5-2x speedup)
load_in_8bit = True  # INT8 quantization

# Stage 5: FlashAttention-2 (1.2-1.5x speedup)
use_flash_attention_2 = True

# Combined: 47s ‚Üí 0.7-1s (50x improvement)
```

### Hardware Requirements

| GPU | VRAM | Batch Size | Expected Performance |
|-----|------|-----------|---------------------|
| **RTX 3060** | 12GB | 16 | 1-2 seconds/sample ‚úÖ Sufficient |
| **RTX 4060/4070** | 8-12GB | 16-32 | 0.8-1.5 seconds/sample |
| **RTX 4090** | 24GB | 32-64 | 0.5-1 seconds/sample |
| **H100** | 80GB | 64+ | 0.5-1 seconds/sample (optimal) |

**Memory Usage:**
- Model (bfloat16): ~3GB
- Model (INT8): ~1.5GB
- Model (INT4): ~0.75GB

---

## üí∞ Cost Analysis

### Data Generation (GPT-4o-mini API)

| Stage | Samples | Cost |
|-------|---------|------|
| Original Generation | 347,685 | $150 |
| Paraphrasing | 222,623 | $200 |
| Feature Variation | 278,148+ | $211 |
| **Total GPT API** | - | **$561** |

### GPU Training (H100 80GB)

| Metric | Value |
|--------|-------|
| Cost per hour | $2-3 |
| Estimated total hours | 720-2,200 |
| **Estimated total cost** | **$1,440-$4,400** |

### Total Project Investment

```
Data Generation:  $561
GPU Training:     $1,440-$4,400
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total:            ~$2,000-$5,000
```

**ROI**: Systematic augmentation strategy successfully addressed data quality issues.

---

## üî¨ Key Research Findings

### 1. Data Quality is Critical

**Discovery**: 85.31% duplication in original dataset
- HTTP_Flood: 97.23% duplicate (worst)
- MQTT_Flood: 97.16% duplicate
- TCP_Flood: 46.19% duplicate (best)
- Normal: 41.16% duplicate

**Impact**: High duplication rate necessitated data augmentation strategy

### 2. Paraphrasing vs Feature Modification

**Findings:** 
- Feature statistics encode critical attack patterns
- Expression diversity provides better data quality
- 80% completion achieved optimal results (quality > quantity)

### 3. Progressive Training Validated

**Benefits:**
- Rapid experimentation (10K trains in hours, not days)
- Resource-efficient evaluation
- Early detection of promising approaches
- Cost savings (avoid training bad models on full data)

### 4. Dataset Version Matters

Our dataset (347,685 samples, 7 classes) differs from published paper (405,296 samples, 8 classes):
- UDP_Flood class completely missing (57,611 samples)
- Normal class severely reduced (57,611 ‚Üí 2,019)
- Total difference: 57,611 samples

See [DATA_MISMATCH_SUMMARY.md](DATA_MISMATCH_SUMMARY.md) for analysis.

---

## üìö Documentation

### Core Documentation
- **[RESEARCH_METHODOLOGY.txt](RESEARCH_METHODOLOGY.txt)**: Complete research pipeline (1,633 lines)
- **[DATA_AUGMENTATION_PROMPTS.txt](DATA_AUGMENTATION_PROMPTS.txt)**: Prompt engineering guide
- **[DATABASE_QUALITY_REPORT.md](DATABASE_QUALITY_REPORT.md)**: Data quality analysis

### Analysis Reports
- [CLASSIFICATION_GUIDE_REPORT.md](CLASSIFICATION_GUIDE_REPORT.md): Classification strategies
- [DATA_MISMATCH_SUMMARY.md](DATA_MISMATCH_SUMMARY.md): Dataset discrepancy analysis
- [FAILED_SAMPLES_ANALYSIS.md](FAILED_SAMPLES_ANALYSIS.md): Error analysis

### Additional Resources
- 30+ markdown reports covering various analysis aspects
- Training logs and evaluation results
- Sample prompts and outputs

---

## üõ†Ô∏è Technologies Used

- **LLM**: Qwen2.5-1.5B-Instruct (Alibaba)
- **Fine-tuning**: LoRA (HuggingFace PEFT)
- **Data Generation**: GPT-4o-mini (OpenAI)
- **AutoML Baseline**: AutoGluon
- **Deep Learning**: PyTorch 2.0+
- **GPU**: NVIDIA H100 80GB (training)
- **Deployment Target**: NVIDIA RTX 3060+ (inference)

---

## üìñ Citation

If you use this work in your research, please cite:

```bibtex
@misc{iot_llm_classification_2025,
  title={Explainable IoT Network Attack Classification with Fine-tuned Large Language Models},
  author={Anonymous},
  year={2025},
  howpublished={\url{https://github.com/[AUTHOR]/iot}}
}
```

**Dataset Source**: Farm-Flow IoT Network Security Dataset (2024)

---

## ü§ù Contributing

This is a research project. For questions or collaboration:
- Open an issue on GitHub
- Contact information will be provided upon paper acceptance

---

## üìÑ License

MIT License - see [LICENSE](LICENSE) file for details

---

## üôè Acknowledgments

- **OpenAI** for GPT-4o-mini API
- **Alibaba** for Qwen2.5 model
- **HuggingFace** for transformers and PEFT libraries
- **AutoGluon** team for AutoML framework
- **Farm-Flow Dataset** authors for IoT security data
- **NVIDIA** for H100 GPU support

---

## üìä Project Status

**Current Stage**: Training in progress (5 models)
- ‚úÖ Data generation complete (347,685 samples)
- ‚úÖ Progressive augmentation complete (both tracks)
- ‚úÖ 10K/30K models training
- ‚è≥ 70K/150K/ALL stages pending
- ‚è≥ Inference optimization pending
- ‚è≥ Production deployment pending

**Next Steps**:
1. Complete current training runs
2. Evaluate model quality and explainability
3. Implement inference optimization (INT8 + FlashAttention)
4. Deploy on RTX 3060 for testing
5. Generate comprehensive benchmarks

---

**Last Updated**: December 15, 2025

